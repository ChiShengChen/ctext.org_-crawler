#!/usr/bin/env python3
"""
å…¨å”è©© N-gram åˆ†æå·¥å…·
çµ±è¨ˆ1-gram, 2-gram, 3-gram, 4-gramé«˜é »è©
"""

import os
import re
import json
from collections import Counter, defaultdict
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class NgramAnalyzer:
    def __init__(self, volumes_dir: str = "quantangshi_volumes"):
        self.volumes_dir = volumes_dir
        self.poems_data = []
        self.ngram_stats = {
            '1gram': Counter(),
            '2gram': Counter(),
            '3gram': Counter(),
            '4gram': Counter()
        }
        
    def load_data(self):
        """è¼‰å…¥æ‰€æœ‰è©©æ­Œæ•¸æ“š"""
        print("ğŸ“š æ­£åœ¨è¼‰å…¥è©©æ­Œæ•¸æ“š...")
        
        for filename in os.listdir(self.volumes_dir):
            if filename.endswith('.txt') and filename.startswith('å…¨å”è©©_ç¬¬'):
                file_path = os.path.join(self.volumes_dir, filename)
                volume_data = self.parse_volume_file(file_path)
                self.poems_data.extend(volume_data)
        
        print(f"âœ… è¼‰å…¥å®Œæˆï¼ç¸½å…± {len(self.poems_data)} é¦–è©©æ­Œ")
        
    def parse_volume_file(self, file_path: str) -> List[Dict]:
        """è§£æå–®å€‹å·æ–‡ä»¶"""
        poems = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æå–å·è™Ÿ
            volume_match = re.search(r'å…¨å”è©©_ç¬¬(\d+)å·', os.path.basename(file_path))
            volume_num = int(volume_match.group(1)) if volume_match else 0
            
            # åˆ†å‰²è©©æ­Œ
            poem_sections = content.split('------------------------------')
            
            for section in poem_sections:
                if not section.strip():
                    continue
                    
                lines = section.strip().split('\n')
                if len(lines) < 3:
                    continue
                
                # æå–æ¨™é¡Œ
                title_line = lines[0].strip()
                if title_line and not title_line.startswith('å…¨å”è©©'):
                    current_poem = {
                        'title': title_line,
                        'volume': volume_num,
                        'content': ''
                    }
                    
                    # æå–ä½œè€…
                    for line in lines[1:]:
                        if 'ä½œè€…:' in line:
                            author = line.split('ä½œè€…:')[1].strip()
                            current_poem['author'] = author
                            break
                    
                    # æå–å…§å®¹
                    content_started = False
                    for line in lines[1:]:
                        if 'å…§å®¹:' in line:
                            content_started = True
                            continue
                        if content_started:
                            current_poem['content'] += line + '\n'
                    
                    if current_poem.get('title') and current_poem.get('author'):
                        poems.append(current_poem)
                        
        except Exception as e:
            print(f"âš ï¸  è§£ææ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
            
        return poems
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œåªä¿ç•™ä¸­æ–‡å­—ç¬¦"""
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿã€æ•¸å­—ã€è‹±æ–‡ç­‰ï¼Œåªä¿ç•™ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        return ''.join(chinese_chars)
    
    def extract_ngrams(self, text: str, n: int) -> List[str]:
        """æå–n-gram"""
        if len(text) < n:
            return []
        
        ngrams = []
        for i in range(len(text) - n + 1):
            ngram = text[i:i+n]
            ngrams.append(ngram)
        
        return ngrams
    
    def analyze_ngrams(self):
        """åˆ†ææ‰€æœ‰n-gram"""
        print("ğŸ” æ­£åœ¨åˆ†æn-gram...")
        
        total_chars = 0
        
        for poem in self.poems_data:
            content = poem.get('content', '')
            cleaned_text = self.clean_text(content)
            total_chars += len(cleaned_text)
            
            # æå–1-4gram
            for n in range(1, 5):
                ngrams = self.extract_ngrams(cleaned_text, n)
                self.ngram_stats[f'{n}gram'].update(ngrams)
        
        print(f"âœ… åˆ†æå®Œæˆï¼ç¸½å…±è™•ç† {total_chars:,} å€‹ä¸­æ–‡å­—ç¬¦")
        
        # æ‰“å°çµ±è¨ˆä¿¡æ¯
        for n in range(1, 5):
            ngram_type = f'{n}gram'
            unique_count = len(self.ngram_stats[ngram_type])
            total_count = sum(self.ngram_stats[ngram_type].values())
            print(f"   {n}-gram: {unique_count:,} å€‹å”¯ä¸€è©ï¼Œ{total_count:,} å€‹ç¸½å‡ºç¾æ¬¡æ•¸")
    
    def get_top_ngrams(self, n: int, top_k: int = 50) -> List[Tuple[str, int]]:
        """ç²å–å‰kå€‹n-gram"""
        ngram_type = f'{n}gram'
        return self.ngram_stats[ngram_type].most_common(top_k)
    
    def save_ngram_results(self, output_file: str = "ngram_analysis_results.txt"):
        """ä¿å­˜n-gramåˆ†æçµæœ"""
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†æçµæœ...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("å…¨å”è©© N-gram åˆ†æçµæœ\n")
            f.write("=" * 50 + "\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for n in range(1, 5):
                ngram_type = f'{n}gram'
                top_ngrams = self.get_top_ngrams(n, 100)
                
                f.write(f"ğŸ“Š {n}-gram é«˜é »è© (å‰100å)\n")
                f.write("-" * 40 + "\n")
                
                for i, (ngram, count) in enumerate(top_ngrams, 1):
                    f.write(f"{i:3d}. '{ngram}': {count:,} æ¬¡\n")
                
                f.write(f"\nç¸½è¨ˆ: {len(self.ngram_stats[ngram_type]):,} å€‹å”¯ä¸€{n}-gram\n")
                f.write(f"ç¸½å‡ºç¾æ¬¡æ•¸: {sum(self.ngram_stats[ngram_type].values()):,}\n\n")
        
        print(f"âœ… åˆ†æçµæœå·²ä¿å­˜åˆ°: {output_file}")
    
    def save_json_results(self, output_file: str = "ngram_analysis.json"):
        """ä¿å­˜JSONæ ¼å¼çš„çµæœ"""
        results = {
            'metadata': {
                'total_poems': len(self.poems_data),
                'generated_at': datetime.now().isoformat()
            },
            'statistics': {},
            'top_ngrams': {}
        }
        
        for n in range(1, 5):
            ngram_type = f'{n}gram'
            top_ngrams = self.get_top_ngrams(n, 100)
            
            results['statistics'][ngram_type] = {
                'unique_count': len(self.ngram_stats[ngram_type]),
                'total_count': sum(self.ngram_stats[ngram_type].values())
            }
            
            results['top_ngrams'][ngram_type] = [
                {'ngram': ngram, 'count': count} 
                for ngram, count in top_ngrams
            ]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONçµæœå·²ä¿å­˜åˆ°: {output_file}")
    
    def create_visualizations(self):
        """å‰µå»ºå¯è¦–åŒ–åœ–è¡¨"""
        print("ğŸ“Š æ­£åœ¨å‰µå»ºå¯è¦–åŒ–åœ–è¡¨...")
        
        # è¨­ç½®åœ–è¡¨æ¨£å¼
        plt.style.use('seaborn-v0_8')
        
        # å‰µå»º2x2çš„å­åœ–
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        
        for n in range(1, 5):
            row = (n - 1) // 2
            col = (n - 1) % 2
            
            top_ngrams = self.get_top_ngrams(n, 20)
            ngrams, counts = zip(*top_ngrams)
            
            # å‰µå»ºæ°´å¹³æ¢å½¢åœ–
            axes[row, col].barh(range(len(ngrams)), counts)
            axes[row, col].set_yticks(range(len(ngrams)))
            axes[row, col].set_yticklabels(ngrams)
            axes[row, col].set_title(f'{n}-gram é«˜é »è© (å‰20å)')
            axes[row, col].set_xlabel('å‡ºç¾æ¬¡æ•¸')
            
            # åœ¨æ¢å½¢åœ–ä¸Šæ·»åŠ æ•¸å€¼æ¨™ç±¤
            for i, count in enumerate(counts):
                axes[row, col].text(count, i, f'{count:,}', 
                                  va='center', ha='left', fontsize=10)
        
        plt.tight_layout()
        plt.savefig('ngram_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜ç‚º: ngram_analysis.png")
    
    def analyze_ngram_patterns(self):
        """åˆ†æn-gramæ¨¡å¼"""
        print("ğŸ” æ­£åœ¨åˆ†æn-gramæ¨¡å¼...")
        
        patterns = {}
        
        for n in range(1, 5):
            ngram_type = f'{n}gram'
            top_ngrams = self.get_top_ngrams(n, 50)
            
            # åˆ†æå­—ç¬¦åˆ†å¸ƒ
            char_freq = Counter()
            for ngram, count in top_ngrams:
                for char in ngram:
                    char_freq[char] += count
            
            patterns[ngram_type] = {
                'top_chars': char_freq.most_common(20),
                'avg_length': n,
                'total_unique': len(self.ngram_stats[ngram_type])
            }
        
        # ä¿å­˜æ¨¡å¼åˆ†æçµæœ
        with open("ngram_patterns.txt", 'w', encoding='utf-8') as f:
            f.write("N-gram æ¨¡å¼åˆ†æ\n")
            f.write("=" * 30 + "\n\n")
            
            for n in range(1, 5):
                ngram_type = f'{n}gram'
                pattern = patterns[ngram_type]
                
                f.write(f"ğŸ“Š {n}-gram æ¨¡å¼åˆ†æ\n")
                f.write("-" * 25 + "\n")
                f.write(f"ç¸½å”¯ä¸€{n}-gramæ•¸: {pattern['total_unique']:,}\n")
                f.write(f"å¹³å‡é•·åº¦: {pattern['avg_length']}\n")
                f.write("é«˜é »å­—ç¬¦ (å‰20å):\n")
                
                for i, (char, count) in enumerate(pattern['top_chars'], 1):
                    f.write(f"  {i:2d}. '{char}': {count:,} æ¬¡\n")
                
                f.write("\n")
        
        print("âœ… æ¨¡å¼åˆ†æçµæœå·²ä¿å­˜åˆ°: ngram_patterns.txt")
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\nğŸ“‹ N-gram åˆ†ææ‘˜è¦:")
        print("=" * 40)
        
        for n in range(1, 5):
            ngram_type = f'{n}gram'
            top_ngrams = self.get_top_ngrams(n, 10)
            
            print(f"\nğŸ”¤ {n}-gram å‰10å:")
            for i, (ngram, count) in enumerate(top_ngrams, 1):
                print(f"   {i:2d}. '{ngram}': {count:,} æ¬¡")
            
            total_unique = len(self.ngram_stats[ngram_type])
            total_count = sum(self.ngram_stats[ngram_type].values())
            print(f"   ç¸½è¨ˆ: {total_unique:,} å€‹å”¯ä¸€{n}-gramï¼Œ{total_count:,} æ¬¡å‡ºç¾")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” å…¨å”è©© N-gram åˆ†æå·¥å…·")
    print("=" * 40)
    
    analyzer = NgramAnalyzer()
    
    # è¼‰å…¥æ•¸æ“š
    analyzer.load_data()
    
    # åˆ†æn-gram
    analyzer.analyze_ngrams()
    
    # ä¿å­˜çµæœ
    analyzer.save_ngram_results()
    analyzer.save_json_results()
    
    # å‰µå»ºå¯è¦–åŒ–
    try:
        analyzer.create_visualizations()
    except Exception as e:
        print(f"âš ï¸  å¯è¦–åŒ–å‰µå»ºå¤±æ•—: {e}")
        print("è«‹ç¢ºä¿å·²å®‰è£ matplotlib å’Œ seaborn")
    
    # åˆ†ææ¨¡å¼
    analyzer.analyze_ngram_patterns()
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()
    
    print("\nğŸ‰ N-gram åˆ†æå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - ngram_analysis_results.txt (è©³ç´°åˆ†æçµæœ)")
    print("   - ngram_analysis.json (JSONæ ¼å¼çµæœ)")
    print("   - ngram_analysis.png (å¯è¦–åŒ–åœ–è¡¨)")
    print("   - ngram_patterns.txt (æ¨¡å¼åˆ†æ)")

if __name__ == "__main__":
    main() 