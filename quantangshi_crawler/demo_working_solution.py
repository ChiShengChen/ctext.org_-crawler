#!/usr/bin/env python3
"""
æ¼”ç¤ºå·¥ä½œè§£æ±ºæ–¹æ¡ˆ
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ”¹é€²çš„çˆ¬èŸ²æˆåŠŸç²å–å…¨å”è©©
"""

import os
import sys
from improved_crawler import ImprovedQuantangshiCrawler

def demo_working_solution():
    """æ¼”ç¤ºå·¥ä½œè§£æ±ºæ–¹æ¡ˆ"""
    print("ğŸ¯ å…¨å”è©©çˆ¬èŸ² - å·¥ä½œè§£æ±ºæ–¹æ¡ˆæ¼”ç¤º")
    print("=" * 50)
    
    # å‰µå»ºæ”¹é€²çš„çˆ¬èŸ²å¯¦ä¾‹
    crawler = ImprovedQuantangshiCrawler(
        config_file="improved_config.json",
        output_dir="demo_output",
        delay=2.0
    )
    
    # æ¸¬è©¦å·²çŸ¥å¯ä»¥å·¥ä½œçš„å·
    test_volumes = [87, 89, 90]  # ç¬¬88å·æœ‰å•é¡Œï¼Œè·³é
    
    print(f"ğŸ“– æ¸¬è©¦å·: {test_volumes}")
    print()
    
    success_count = 0
    total_poems = 0
    
    for volume in test_volumes:
        print(f"ğŸ”„ æ­£åœ¨è™•ç†ç¬¬ {volume} å·...")
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡ä»¶
        filename = f"å…¨å”è©©_ç¬¬{volume:03d}å·.txt"
        filepath = os.path.join(crawler.output_dir, filename)
        
        if os.path.exists(filepath):
            print(f"   âš ï¸  æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³é")
            continue
        
        # ç²å–è©©æ­Œ
        poems, status = crawler.fetch_volume_with_retry(volume)
        
        if status == "success":
            print(f"   âœ… æˆåŠŸï¼ç²å–åˆ° {len(poems)} é¦–è©©")
            crawler.save_volume_to_file(poems, volume)
            success_count += 1
            total_poems += len(poems)
            
            # é¡¯ç¤ºå‰3é¦–è©©çš„æ¨™é¡Œ
            print("   ğŸ“ å‰3é¦–è©©:")
            for i, poem in enumerate(poems[:3], 1):
                title = poem.get('title', 'æœªçŸ¥æ¨™é¡Œ')
                author = poem.get('author', 'æœªçŸ¥ä½œè€…')
                print(f"      {i}. {title} - {author}")
        else:
            print(f"   âŒ å¤±æ•—: {status}")
        
        print()
    
    # ç¸½çµ
    print("=" * 50)
    print("ğŸ“Š æ¼”ç¤ºçµæœ:")
    print(f"   âœ… æˆåŠŸè™•ç†: {success_count}/{len(test_volumes)} å·")
    print(f"   ğŸ“ ç¸½è©©æ­Œæ•¸: {total_poems} é¦–")
    print(f"   ğŸ’¾ è¼¸å‡ºç›®éŒ„: {crawler.output_dir}")
    
    if success_count > 0:
        print("\nğŸ‰ æ¼”ç¤ºæˆåŠŸï¼æ”¹é€²çš„çˆ¬èŸ²å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print("   1. å°æ–¼æ­£å¸¸å·ï¼Œä½¿ç”¨ improved_crawler.py")
        print("   2. å°æ–¼å•é¡Œå·ï¼Œä½¿ç”¨ advanced_crawler.py")
        print("   3. åˆ†æ‰¹è™•ç†ï¼Œé¿å…ä¸€æ¬¡æ€§çˆ¬å–å¤ªå¤šå·")
        print("   4. å®šæœŸé‡è©¦å¤±æ•—çš„å·")
    else:
        print("\nâš ï¸  æ¼”ç¤ºå¤±æ•—ï¼Œå¯èƒ½éœ€è¦èª¿æ•´é…ç½®æˆ–ç­‰å¾…æ›´é•·æ™‚é–“ã€‚")

def show_file_structure():
    """é¡¯ç¤ºæ–‡ä»¶çµæ§‹"""
    print("\nğŸ“ é …ç›®æ–‡ä»¶çµæ§‹:")
    print("   improved_crawler.py      - æ”¹é€²ç‰ˆçˆ¬èŸ²")
    print("   advanced_crawler.py      - é«˜ç´šçˆ¬èŸ²")
    print("   improved_config.json     - æ”¹é€²ç‰ˆé…ç½®")
    print("   advanced_config.json     - é«˜ç´šé…ç½®")
    print("   test_crawler.py          - æ¸¬è©¦è…³æœ¬")
    print("   retry_failed_volumes.py  - é‡è©¦è…³æœ¬")
    print("   DEBUG_SUMMARY.md         - èª¿è©¦ç¸½çµ")

if __name__ == "__main__":
    demo_working_solution()
    show_file_structure() 