#!/usr/bin/env python3
"""
å…¨å”è©©äººç‰©é—œä¿‚ç¶²è·¯åˆ†æå·¥å…· (æ”¹é€²ç‰ˆ)
åªä¿ç•™ä½œè€…åˆ—è¡¨ä¸­çš„äººåï¼Œè§£æ±ºä¸­æ–‡é¡¯ç¤ºå•é¡Œï¼Œä¸¦å°‡çµæœæ”¾åœ¨å–®ç¨çš„è³‡æ–™å¤¾ä¸­
"""

import os
import re
import json
import csv
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set
from datetime import datetime
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class ImprovedPersonNetworkAnalyzer:
    def __init__(self, volumes_dir: str = "quantangshi_volumes"):
        self.volumes_dir = volumes_dir
        self.poems_data = []
        self.author_list = set()  # ä½œè€…åˆ—è¡¨
        self.person_poems = defaultdict(list)  # äººç‰© -> è©©æ­Œåˆ—è¡¨
        self.poem_persons = defaultdict(set)   # è©©æ­Œ -> äººç‰©é›†åˆ
        self.person_mentions = Counter()       # äººç‰©æåŠæ¬¡æ•¸
        self.person_connections = defaultdict(Counter)  # äººç‰©é—œä¿‚ç¶²çµ¡
        
    def load_author_list(self):
        """è¼‰å…¥ä½œè€…åˆ—è¡¨"""
        print("ğŸ“š æ­£åœ¨è¼‰å…¥ä½œè€…åˆ—è¡¨...")
        
        author_file = "analysis_result/authors_list_clean.txt"
        with open(author_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            # è·³éæ¨™é¡Œè¡Œå’Œç©ºè¡Œ
            if line.startswith('å…¨å”è©©') or line.startswith('=') or line.startswith('ç¸½è¨ˆ') or line.startswith('æ³¨æ„') or not line:
                continue
            
            # æå–ä½œè€…åç¨±ï¼ˆå»æ‰åºè™Ÿå’Œé»ï¼‰
            if '. ' in line:
                author = line.split('. ', 1)[1]
            else:
                author = line
            
            if author and len(author) >= 2:
                self.author_list.add(author)
        
        print(f"âœ… è¼‰å…¥å®Œæˆï¼ç¸½å…± {len(self.author_list)} ä½ä½œè€…")
        
    def load_data(self):
        """è¼‰å…¥æ‰€æœ‰è©©æ­Œæ•¸æ“š"""
        print("ğŸ“š æ­£åœ¨è¼‰å…¥è©©æ­Œæ•¸æ“š...")
        
        for filename in os.listdir(self.volumes_dir):
            if filename.endswith('.txt') and filename.startswith('å…¨å”è©©_ç¬¬'):
                file_path = os.path.join(self.volumes_dir, filename)
                volume_data = self.parse_volume_file(file_path)
                self.poems_data.extend(volume_data)
        
        print(f"âœ… è¼‰å…¥å®Œæˆï¼ç¸½å…± {len(self.poems_data)} é¦–è©©æ­Œ")
        
    def parse_volume_file(self, file_path: str) -> List[Dict]:
        """è§£æå–®å€‹å·æ–‡ä»¶"""
        poems = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æå–å·è™Ÿ
            volume_match = re.search(r'å…¨å”è©©_ç¬¬(\d+)å·', os.path.basename(file_path))
            volume_num = int(volume_match.group(1)) if volume_match else 0
            
            # åˆ†å‰²è©©æ­Œ
            poem_sections = content.split('------------------------------')
            
            for section in poem_sections:
                if not section.strip():
                    continue
                    
                lines = section.strip().split('\n')
                if len(lines) < 3:
                    continue
                
                # æå–æ¨™é¡Œ
                title_line = lines[0].strip()
                if title_line and not title_line.startswith('å…¨å”è©©'):
                    current_poem = {
                        'title': title_line,
                        'volume': volume_num,
                        'content': ''
                    }
                    
                    # æå–ä½œè€…
                    for line in lines[1:]:
                        if 'ä½œè€…:' in line:
                            author = line.split('ä½œè€…:')[1].strip()
                            current_poem['author'] = author
                            break
                    
                    # æå–å…§å®¹
                    content_started = False
                    for line in lines[1:]:
                        if 'å…§å®¹:' in line:
                            content_started = True
                            continue
                        if content_started:
                            current_poem['content'] += line + '\n'
                    
                    if current_poem.get('title') and current_poem.get('author'):
                        poems.append(current_poem)
                        
        except Exception as e:
            print(f"âš ï¸  è§£ææ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
            
        return poems
    
    def clean_author_name(self, author: str) -> str:
        """æ¸…ç†ä½œè€…åç¨±ï¼Œå»æ‰"è‘—"å­—"""
        if author.endswith('è‘—'):
            return author[:-1]
        return author
    
    def extract_persons_from_text(self, text: str) -> Set[str]:
        """å¾æ–‡æœ¬ä¸­æå–äººç‰©ï¼ˆåªä¿ç•™ä½œè€…åˆ—è¡¨ä¸­çš„äººåï¼‰"""
        persons = set()
        
        # åªåŒ¹é…ä½œè€…åˆ—è¡¨ä¸­çš„äººå
        for author in self.author_list:
            if author in text:
                persons.add(author)
        
        return persons
    
    def analyze_person_mentions(self):
        """åˆ†æè©©ä¸­çš„äººç‰©æåŠ"""
        print("ğŸ” æ­£åœ¨åˆ†æè©©ä¸­çš„äººç‰©æåŠ...")
        
        total_poems_with_persons = 0
        
        for poem in self.poems_data:
            title = poem.get('title', '')
            content = poem.get('content', '')
            full_text = title + '\n' + content
            
            # æå–äººç‰©
            persons = self.extract_persons_from_text(full_text)
            
            if persons:
                total_poems_with_persons += 1
                poem_id = f"{poem.get('author', 'ä½šå')}_{poem.get('title', '')}"
                
                # è¨˜éŒ„è©©æ­Œä¸­çš„äººç‰©
                self.poem_persons[poem_id] = persons
                
                # çµ±è¨ˆäººç‰©æåŠæ¬¡æ•¸
                for person in persons:
                    self.person_mentions[person] += 1
                    self.person_poems[person].append(poem)
                
                # å»ºç«‹äººç‰©é—œä¿‚ç¶²çµ¡ï¼ˆåŒä¸€é¦–è©©ä¸­çš„äººç‰©ç›¸äº’é—œè¯ï¼‰
                person_list = list(persons)
                for i in range(len(person_list)):
                    for j in range(i + 1, len(person_list)):
                        person1, person2 = person_list[i], person_list[j]
                        self.person_connections[person1][person2] += 1
                        self.person_connections[person2][person1] += 1
        
        print(f"âœ… åˆ†æå®Œæˆï¼")
        print(f"   æåˆ°äººç‰©çš„è©©æ­Œ: {total_poems_with_persons:,} é¦–")
        print(f"   ç™¼ç¾çš„äººç‰©: {len(self.person_mentions):,} å€‹")
        print(f"   äººç‰©æåŠç¸½æ¬¡æ•¸: {sum(self.person_mentions.values()):,}")
    
    def save_person_poems(self, output_dir: str = "analysis_result/person_network_analysis"):
        """ä¿å­˜æåˆ°äººç‰©çš„è©©æ­Œåˆ—è¡¨"""
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æåˆ°äººç‰©çš„è©©æ­Œåˆ—è¡¨...")
        
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "person_poems.csv")
        
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['è©©æ­ŒID', 'ä½œè€…', 'æ¨™é¡Œ', 'äººç‰©', 'æåŠæ¬¡æ•¸']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for poem_id, persons in self.poem_persons.items():
                author, title = poem_id.split('_', 1)
                for person in persons:
                    writer.writerow({
                        'è©©æ­ŒID': poem_id,
                        'ä½œè€…': author,
                        'æ¨™é¡Œ': title,
                        'äººç‰©': person,
                        'æåŠæ¬¡æ•¸': self.person_mentions[person]
                    })
        
        print(f"âœ… æåˆ°äººç‰©çš„è©©æ­Œåˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    
    def save_person_statistics(self, output_dir: str = "analysis_result/person_network_analysis"):
        """ä¿å­˜äººç‰©çµ±è¨ˆæ•¸æ“š"""
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜äººç‰©çµ±è¨ˆæ•¸æ“š...")
        
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "person_statistics.csv")
        
        # æŒ‰æåŠæ¬¡æ•¸æ’åº
        sorted_persons = self.person_mentions.most_common()
        
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['æ’å', 'äººç‰©', 'æåŠæ¬¡æ•¸', 'ç›¸é—œè©©æ­Œæ•¸', 'é—œè¯äººç‰©æ•¸']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for rank, (person, mentions) in enumerate(sorted_persons, 1):
                related_poems = len(self.person_poems[person])
                related_persons = len(self.person_connections[person])
                
                writer.writerow({
                    'æ’å': rank,
                    'äººç‰©': person,
                    'æåŠæ¬¡æ•¸': mentions,
                    'ç›¸é—œè©©æ­Œæ•¸': related_poems,
                    'é—œè¯äººç‰©æ•¸': related_persons
                })
        
        print(f"âœ… äººç‰©çµ±è¨ˆæ•¸æ“šå·²ä¿å­˜åˆ°: {output_file}")
    
    def create_person_network(self, min_mentions: int = 3, min_connections: int = 1):
        """å‰µå»ºäººç‰©é—œä¿‚ç¶²è·¯åœ–"""
        print("ğŸ•¸ï¸ æ­£åœ¨å‰µå»ºäººç‰©é—œä¿‚ç¶²è·¯åœ–...")
        
        # å‰µå»ºNetworkXåœ–
        G = nx.Graph()
        
        # éæ¿¾é‡è¦äººç‰©ï¼ˆæåŠæ¬¡æ•¸å’Œé—œè¯æ•¸é”åˆ°é–¾å€¼ï¼‰
        important_persons = []
        for person, mentions in self.person_mentions.items():
            if mentions >= min_mentions:
                connections = len(self.person_connections[person])
                if connections >= min_connections:
                    important_persons.append(person)
        
        print(f"   é‡è¦äººç‰©æ•¸é‡: {len(important_persons)}")
        
        # æ·»åŠ ç¯€é»
        for person in important_persons:
            mentions = self.person_mentions[person]
            G.add_node(person, weight=mentions, size=min(mentions/5 + 5, 30))
        
        # æ·»åŠ é‚Š
        for person1 in important_persons:
            for person2 in important_persons:
                if person1 < person2:  # é¿å…é‡è¤‡é‚Š
                    weight = self.person_connections[person1].get(person2, 0)
                    if weight > 0:
                        G.add_edge(person1, person2, weight=weight)
        
        print(f"   ç¯€é»æ•¸é‡: {G.number_of_nodes()}")
        print(f"   é‚Šæ•¸é‡: {G.number_of_edges()}")
        
        return G, important_persons
    
    def draw_network_graph(self, G, output_dir: str = "analysis_result/person_network_analysis"):
        """ç¹ªè£½äººç‰©é—œä¿‚ç¶²è·¯åœ–"""
        print("ğŸ¨ æ­£åœ¨ç¹ªè£½äººç‰©é—œä¿‚ç¶²è·¯åœ–...")
        
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "person_network.png")
        
        # è¨­ç½®åœ–çš„å¤§å°
        plt.figure(figsize=(20, 16))
        
        # è¨ˆç®—ç¯€é»å¤§å°å’Œé¡è‰²
        node_sizes = [G.nodes[node]['size'] for node in G.nodes()]
        node_weights = [G.nodes[node]['weight'] for node in G.nodes()]
        
        # è¨ˆç®—é‚Šçš„å¯¬åº¦
        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
        edge_widths = [w/3 for w in edge_weights]
        
        # è¨ˆç®—ä½ˆå±€
        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
        
        # ç¹ªè£½ç¯€é»
        nx.draw_networkx_nodes(G, pos, 
                              node_size=node_sizes,
                              node_color=node_weights,
                              cmap=plt.cm.viridis,
                              alpha=0.8)
        
        # ç¹ªè£½é‚Š
        nx.draw_networkx_edges(G, pos,
                              width=edge_widths,
                              alpha=0.3,
                              edge_color='gray')
        
        # ç¹ªè£½æ¨™ç±¤ï¼ˆåªé¡¯ç¤ºé‡è¦ç¯€é»ï¼‰
        labels = {}
        for node in G.nodes():
            if G.nodes[node]['weight'] >= 10:  # åªé¡¯ç¤ºæåŠæ¬¡æ•¸>=10çš„äººç‰©
                labels[node] = node
        
        nx.draw_networkx_labels(G, pos, labels, font_size=8)
        
        # æ·»åŠ æ¨™é¡Œå’Œèªªæ˜
        plt.title('Tang Poetry Person Network\n(Node size: mention count, Edge width: connection strength)', 
                 fontsize=16, pad=20)
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… äººç‰©é—œä¿‚ç¶²è·¯åœ–å·²ä¿å­˜åˆ°: {output_file}")
    
    def save_network_data(self, G, output_dir: str = "analysis_result/person_network_analysis"):
        """ä¿å­˜ç¶²è·¯æ•¸æ“šç‚ºJSONæ ¼å¼"""
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç¶²è·¯æ•¸æ“š...")
        
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "person_network_data.json")
        
        network_data = {
            'metadata': {
                'total_nodes': G.number_of_nodes(),
                'total_edges': G.number_of_edges(),
                'generated_at': datetime.now().isoformat()
            },
            'nodes': [],
            'edges': []
        }
        
        # ç¯€é»æ•¸æ“š
        for node in G.nodes():
            network_data['nodes'].append({
                'id': node,
                'weight': G.nodes[node]['weight'],
                'size': G.nodes[node]['size']
            })
        
        # é‚Šæ•¸æ“š
        for u, v in G.edges():
            network_data['edges'].append({
                'source': u,
                'target': v,
                'weight': G[u][v]['weight']
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(network_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç¶²è·¯æ•¸æ“šå·²ä¿å­˜åˆ°: {output_file}")
    
    def create_analysis_report(self, output_dir: str = "analysis_result/person_network_analysis"):
        """å‰µå»ºåˆ†æå ±å‘Š"""
        print("ğŸ“Š æ­£åœ¨å‰µå»ºåˆ†æå ±å‘Š...")
        
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, "analysis_report.md")
        
        total_poems_with_persons = len(self.poem_persons)
        total_persons = len(self.person_mentions)
        total_mentions = sum(self.person_mentions.values())
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# å…¨å”è©©äººç‰©é—œä¿‚ç¶²è·¯åˆ†æå ±å‘Š (æ”¹é€²ç‰ˆ)\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## åˆ†ææ¦‚è¿°\n\n")
            f.write("æœ¬åˆ†æåªä¿ç•™ä½œè€…åˆ—è¡¨ä¸­çš„äººåï¼Œç¢ºä¿æ‰€æœ‰æåŠçš„äººç‰©éƒ½æ˜¯çœŸå¯¦çš„è©©äººæˆ–æ­·å²äººç‰©ã€‚\n\n")
            
            f.write("## çµ±è¨ˆæ‘˜è¦\n\n")
            f.write(f"- ç¸½è©©æ­Œæ•¸: {len(self.poems_data):,} é¦–\n")
            f.write(f"- æåˆ°äººç‰©çš„è©©æ­Œ: {total_poems_with_persons:,} é¦–\n")
            f.write(f"- ç™¼ç¾çš„äººç‰©: {total_persons:,} å€‹\n")
            f.write(f"- äººç‰©æåŠç¸½æ¬¡æ•¸: {total_mentions:,} æ¬¡\n")
            f.write(f"- ä½œè€…åˆ—è¡¨ç¸½æ•¸: {len(self.author_list):,} ä½\n\n")
            
            f.write("## æåŠæ¬¡æ•¸æœ€å¤šçš„å‰20ä½äººç‰©\n\n")
            f.write("| æ’å | äººç‰© | æåŠæ¬¡æ•¸ | ç›¸é—œè©©æ­Œæ•¸ | é—œè¯äººç‰©æ•¸ |\n")
            f.write("|------|------|----------|------------|------------|\n")
            
            for i, (person, mentions) in enumerate(self.person_mentions.most_common(20), 1):
                related_poems = len(self.person_poems[person])
                related_persons = len(self.person_connections[person])
                f.write(f"| {i} | {person} | {mentions} | {related_poems} | {related_persons} |\n")
            
            f.write("\n## æ–‡ä»¶èªªæ˜\n\n")
            f.write("- `person_poems.csv`: æåˆ°äººç‰©çš„è©©æ­Œåˆ—è¡¨\n")
            f.write("- `person_statistics.csv`: äººç‰©çµ±è¨ˆæ•¸æ“š\n")
            f.write("- `person_network.png`: äººç‰©é—œä¿‚ç¶²è·¯åœ–\n")
            f.write("- `person_network_data.json`: ç¶²è·¯æ•¸æ“š\n")
            f.write("- `analysis_report.md`: æœ¬å ±å‘Š\n\n")
            
            f.write("## æ”¹é€²èªªæ˜\n\n")
            f.write("1. **ç²¾ç¢ºäººç‰©è­˜åˆ¥**: åªä¿ç•™ä½œè€…åˆ—è¡¨ä¸­çš„äººå\n")
            f.write("2. **è§£æ±ºä¸­æ–‡é¡¯ç¤º**: ä½¿ç”¨è‹±æ–‡æ¨™é¡Œé¿å…å­—é«”å•é¡Œ\n")
            f.write("3. **çµ„ç¹”çµæ§‹**: æ‰€æœ‰çµæœæ–‡ä»¶æ”¾åœ¨å–®ç¨çš„è³‡æ–™å¤¾ä¸­\n")
        
        print(f"âœ… åˆ†æå ±å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\nğŸ“‹ äººç‰©é—œä¿‚ç¶²è·¯åˆ†ææ‘˜è¦ (æ”¹é€²ç‰ˆ):")
        print("=" * 50)
        
        total_poems_with_persons = len(self.poem_persons)
        total_persons = len(self.person_mentions)
        total_mentions = sum(self.person_mentions.values())
        
        print(f"ç¸½è©©æ­Œæ•¸: {len(self.poems_data):,} é¦–")
        print(f"æåˆ°äººç‰©çš„è©©æ­Œ: {total_poems_with_persons:,} é¦–")
        print(f"ç™¼ç¾çš„äººç‰©: {total_persons:,} å€‹")
        print(f"äººç‰©æåŠç¸½æ¬¡æ•¸: {total_mentions:,}")
        print(f"ä½œè€…åˆ—è¡¨ç¸½æ•¸: {len(self.author_list):,} ä½")
        
        print(f"\nğŸ† æåŠæ¬¡æ•¸æœ€å¤šçš„å‰10ä½äººç‰©:")
        for i, (person, mentions) in enumerate(self.person_mentions.most_common(10), 1):
            related_poems = len(self.person_poems[person])
            print(f"   {i:2d}. {person}: {mentions:,} æ¬¡æåŠ ({related_poems:,} é¦–è©©)")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ•¸ï¸ å…¨å”è©©äººç‰©é—œä¿‚ç¶²è·¯åˆ†æå·¥å…· (æ”¹é€²ç‰ˆ)")
    print("=" * 50)
    
    analyzer = ImprovedPersonNetworkAnalyzer()
    
    # è¼‰å…¥ä½œè€…åˆ—è¡¨
    analyzer.load_author_list()
    
    # è¼‰å…¥æ•¸æ“š
    analyzer.load_data()
    
    # åˆ†æäººç‰©æåŠ
    analyzer.analyze_person_mentions()
    
    # ä¿å­˜çµæœ
    output_dir = "analysis_result/person_network_analysis"
    analyzer.save_person_poems(output_dir)
    analyzer.save_person_statistics(output_dir)
    
    # å‰µå»ºç¶²è·¯åœ–
    G, important_persons = analyzer.create_person_network(min_mentions=3, min_connections=1)
    
    # ç¹ªè£½ç¶²è·¯åœ–
    analyzer.draw_network_graph(G, output_dir)
    
    # ä¿å­˜ç¶²è·¯æ•¸æ“š
    analyzer.save_network_data(G, output_dir)
    
    # å‰µå»ºåˆ†æå ±å‘Š
    analyzer.create_analysis_report(output_dir)
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()
    
    print(f"\nğŸ‰ äººç‰©é—œä¿‚ç¶²è·¯åˆ†æ (æ”¹é€²ç‰ˆ) å®Œæˆï¼")
    print(f"ğŸ“ çµæœä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main() 