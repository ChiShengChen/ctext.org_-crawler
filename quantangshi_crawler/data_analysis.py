#!/usr/bin/env python3
"""
å…¨å”è©©æ•¸æ“šåˆ†æå·¥å…·
ç”¨æ–¼åˆ†æå·²çˆ¬å–çš„å…¨å”è©©æ•¸æ“šï¼Œç”Ÿæˆçµ±è¨ˆå ±å‘Šå’Œå¯è¦–åŒ–
"""

import os
import json
import re
from collections import Counter, defaultdict
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class QuantangshiAnalyzer:
    def __init__(self, volumes_dir: str = "quantangshi_volumes"):
        self.volumes_dir = volumes_dir
        self.poems_data = []
        self.authors_data = {}
        self.volumes_data = {}
        
    def load_data(self):
        """è¼‰å…¥æ‰€æœ‰è©©æ­Œæ•¸æ“š"""
        print("ğŸ“š æ­£åœ¨è¼‰å…¥è©©æ­Œæ•¸æ“š...")
        
        for filename in os.listdir(self.volumes_dir):
            if filename.endswith('.txt') and filename.startswith('å…¨å”è©©_ç¬¬'):
                file_path = os.path.join(self.volumes_dir, filename)
                volume_data = self.parse_volume_file(file_path)
                self.poems_data.extend(volume_data)
        
        print(f"âœ… è¼‰å…¥å®Œæˆï¼ç¸½å…± {len(self.poems_data)} é¦–è©©æ­Œ")
        
    def parse_volume_file(self, file_path: str) -> List[Dict]:
        """è§£æå–®å€‹å·æ–‡ä»¶"""
        poems = []
        current_poem = {}
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æå–å·è™Ÿ
            volume_match = re.search(r'å…¨å”è©©_ç¬¬(\d+)å·', os.path.basename(file_path))
            volume_num = int(volume_match.group(1)) if volume_match else 0
            
            # åˆ†å‰²è©©æ­Œ
            poem_sections = content.split('------------------------------')
            
            for section in poem_sections:
                if not section.strip():
                    continue
                    
                lines = section.strip().split('\n')
                if len(lines) < 3:
                    continue
                
                # æå–æ¨™é¡Œ
                title_line = lines[0].strip()
                if title_line and not title_line.startswith('å…¨å”è©©'):
                    current_poem = {
                        'title': title_line,
                        'volume': volume_num,
                        'content': ''
                    }
                    
                    # æå–ä½œè€…
                    for line in lines[1:]:
                        if 'ä½œè€…:' in line:
                            author = line.split('ä½œè€…:')[1].strip()
                            current_poem['author'] = author
                            break
                    
                    # æå–å…§å®¹
                    content_started = False
                    for line in lines[1:]:
                        if 'å…§å®¹:' in line:
                            content_started = True
                            continue
                        if content_started:
                            current_poem['content'] += line + '\n'
                    
                    if current_poem.get('title') and current_poem.get('author'):
                        poems.append(current_poem)
                        
        except Exception as e:
            print(f"âš ï¸  è§£ææ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
            
        return poems
    
    def analyze_authors(self) -> Dict:
        """åˆ†æä½œè€…çµ±è¨ˆ"""
        print("ğŸ‘¥ æ­£åœ¨åˆ†æä½œè€…çµ±è¨ˆ...")
        
        author_counts = Counter()
        author_poems = defaultdict(list)
        
        for poem in self.poems_data:
            author = poem.get('author', 'ä½šå')
            author_counts[author] += 1
            author_poems[author].append(poem)
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        total_authors = len(author_counts)
        total_poems = len(self.poems_data)
        avg_poems_per_author = total_poems / total_authors if total_authors > 0 else 0
        
        # åªå‡ºç¾ä¸€æ¬¡çš„ä½œè€…
        single_poem_authors = sum(1 for count in author_counts.values() if count == 1)
        
        self.authors_data = {
            'total_authors': total_authors,
            'total_poems': total_poems,
            'avg_poems_per_author': avg_poems_per_author,
            'single_poem_authors': single_poem_authors,
            'top_authors': author_counts.most_common(20),
            'author_poems': dict(author_poems)
        }
        
        return self.authors_data
    
    def analyze_volumes(self) -> Dict:
        """åˆ†æå·çµ±è¨ˆ"""
        print("ğŸ“– æ­£åœ¨åˆ†æå·çµ±è¨ˆ...")
        
        volume_counts = Counter()
        volume_poems = defaultdict(list)
        
        for poem in self.poems_data:
            volume = poem.get('volume', 0)
            volume_counts[volume] += 1
            volume_poems[volume].append(poem)
        
        self.volumes_data = {
            'total_volumes': len(volume_counts),
            'volume_poem_counts': dict(volume_counts),
            'volume_poems': dict(volume_poems),
            'avg_poems_per_volume': len(self.poems_data) / len(volume_counts) if volume_counts else 0
        }
        
        return self.volumes_data
    
    def analyze_content(self) -> Dict:
        """åˆ†æå…§å®¹çµ±è¨ˆ"""
        print("ğŸ“ æ­£åœ¨åˆ†æå…§å®¹çµ±è¨ˆ...")
        
        content_stats = {
            'total_characters': 0,
            'avg_poem_length': 0,
            'longest_poem': None,
            'shortest_poem': None,
            'common_words': Counter()
        }
        
        poem_lengths = []
        
        for poem in self.poems_data:
            content = poem.get('content', '')
            char_count = len(content)
            poem_lengths.append(char_count)
            content_stats['total_characters'] += char_count
            
            # ç°¡å–®çš„è©é »çµ±è¨ˆï¼ˆæŒ‰å­—ç¬¦ï¼‰
            for char in content:
                if char.strip():
                    content_stats['common_words'][char] += 1
        
        if poem_lengths:
            content_stats['avg_poem_length'] = sum(poem_lengths) / len(poem_lengths)
            
            # æœ€é•·å’Œæœ€çŸ­çš„è©©
            max_length = max(poem_lengths)
            min_length = min(poem_lengths)
            
            for poem in self.poems_data:
                if len(poem.get('content', '')) == max_length:
                    content_stats['longest_poem'] = poem
                if len(poem.get('content', '')) == min_length:
                    content_stats['shortest_poem'] = poem
        
        return content_stats
    
    def generate_report(self, output_file: str = "analysis_report.txt"):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆåˆ†æå ±å‘Š...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("å…¨å”è©©æ•¸æ“šåˆ†æå ±å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # åŸºæœ¬çµ±è¨ˆ
            f.write("ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ\n")
            f.write("-" * 20 + "\n")
            f.write(f"ç¸½è©©æ­Œæ•¸: {self.authors_data['total_poems']}\n")
            f.write(f"ç¸½ä½œè€…æ•¸: {self.authors_data['total_authors']}\n")
            f.write(f"ç¸½å·æ•¸: {self.volumes_data['total_volumes']}\n")
            f.write(f"å¹³å‡æ¯å·è©©æ­Œæ•¸: {self.volumes_data['avg_poems_per_volume']:.1f}\n")
            f.write(f"å¹³å‡æ¯ä½ä½œè€…è©©æ­Œæ•¸: {self.authors_data['avg_poems_per_author']:.1f}\n")
            f.write(f"åªå‡ºç¾ä¸€æ¬¡çš„ä½œè€…: {self.authors_data['single_poem_authors']}\n\n")
            
            # æœ€æ´»èºä½œè€…
            f.write("ğŸ† æœ€æ´»èºçš„ä½œè€… (å‰20å)\n")
            f.write("-" * 30 + "\n")
            for i, (author, count) in enumerate(self.authors_data['top_authors'], 1):
                f.write(f"{i:2d}. {author}: {count} é¦–\n")
            f.write("\n")
            
            # å…§å®¹çµ±è¨ˆ
            content_stats = self.analyze_content()
            f.write("ğŸ“ å…§å®¹çµ±è¨ˆ\n")
            f.write("-" * 15 + "\n")
            f.write(f"ç¸½å­—ç¬¦æ•¸: {content_stats['total_characters']:,}\n")
            f.write(f"å¹³å‡è©©æ­Œé•·åº¦: {content_stats['avg_poem_length']:.1f} å­—ç¬¦\n")
            
            if content_stats['longest_poem']:
                f.write(f"æœ€é•·è©©æ­Œ: {content_stats['longest_poem']['title']} (ä½œè€…: {content_stats['longest_poem']['author']})\n")
            if content_stats['shortest_poem']:
                f.write(f"æœ€çŸ­è©©æ­Œ: {content_stats['shortest_poem']['title']} (ä½œè€…: {content_stats['shortest_poem']['author']})\n")
            
            f.write("\n")
            
            # æœ€å¸¸è¦‹å­—ç¬¦
            f.write("ğŸ”¤ æœ€å¸¸è¦‹å­—ç¬¦ (å‰20å€‹)\n")
            f.write("-" * 25 + "\n")
            for char, count in content_stats['common_words'].most_common(20):
                f.write(f"'{char}': {count:,} æ¬¡\n")
        
        print(f"âœ… åˆ†æå ±å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def create_visualizations(self):
        """å‰µå»ºå¯è¦–åŒ–åœ–è¡¨"""
        print("ğŸ“Š æ­£åœ¨å‰µå»ºå¯è¦–åŒ–åœ–è¡¨...")
        
        # è¨­ç½®åœ–è¡¨æ¨£å¼
        plt.style.use('seaborn-v0_8')
        
        # 1. ä½œè€…è©©æ­Œæ•¸é‡åˆ†å¸ƒ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å‰20åä½œè€…çš„è©©æ­Œæ•¸é‡
        top_authors = self.authors_data['top_authors'][:20]
        authors, counts = zip(*top_authors)
        
        axes[0, 0].barh(range(len(authors)), counts)
        axes[0, 0].set_yticks(range(len(authors)))
        axes[0, 0].set_yticklabels(authors)
        axes[0, 0].set_title('å‰20åä½œè€…çš„è©©æ­Œæ•¸é‡')
        axes[0, 0].set_xlabel('è©©æ­Œæ•¸é‡')
        
        # è©©æ­Œé•·åº¦åˆ†å¸ƒ
        poem_lengths = [len(poem.get('content', '')) for poem in self.poems_data]
        axes[0, 1].hist(poem_lengths, bins=50, alpha=0.7)
        axes[0, 1].set_title('è©©æ­Œé•·åº¦åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('å­—ç¬¦æ•¸')
        axes[0, 1].set_ylabel('è©©æ­Œæ•¸é‡')
        
        # å·è™Ÿèˆ‡è©©æ­Œæ•¸é‡
        volume_data = sorted(self.volumes_data['volume_poem_counts'].items())
        volumes, poem_counts = zip(*volume_data)
        
        axes[1, 0].plot(volumes, poem_counts, alpha=0.7)
        axes[1, 0].set_title('å„å·è©©æ­Œæ•¸é‡')
        axes[1, 0].set_xlabel('å·è™Ÿ')
        axes[1, 0].set_ylabel('è©©æ­Œæ•¸é‡')
        
        # ä½œè€…æ´»èºåº¦åˆ†å¸ƒ
        author_counts = [count for _, count in self.authors_data['top_authors']]
        axes[1, 1].hist(author_counts, bins=30, alpha=0.7)
        axes[1, 1].set_title('ä½œè€…æ´»èºåº¦åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('è©©æ­Œæ•¸é‡')
        axes[1, 1].set_ylabel('ä½œè€…æ•¸é‡')
        
        plt.tight_layout()
        plt.savefig('quantangshi_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜ç‚º: quantangshi_analysis.png")
    
    def export_json_data(self, output_file: str = "quantangshi_data.json"):
        """å°å‡ºæ•¸æ“šç‚ºJSONæ ¼å¼"""
        print("ğŸ’¾ æ­£åœ¨å°å‡ºJSONæ•¸æ“š...")
        
        export_data = {
            'metadata': {
                'total_poems': len(self.poems_data),
                'total_authors': self.authors_data['total_authors'],
                'total_volumes': self.volumes_data['total_volumes'],
                'generated_at': datetime.now().isoformat()
            },
            'poems': self.poems_data,
            'authors': {
                'statistics': {
                    'total_authors': self.authors_data['total_authors'],
                    'avg_poems_per_author': self.authors_data['avg_poems_per_author'],
                    'single_poem_authors': self.authors_data['single_poem_authors']
                },
                'top_authors': dict(self.authors_data['top_authors'])
            },
            'volumes': {
                'statistics': {
                    'total_volumes': self.volumes_data['total_volumes'],
                    'avg_poems_per_volume': self.volumes_data['avg_poems_per_volume']
                },
                'volume_poem_counts': self.volumes_data['volume_poem_counts']
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæ•¸æ“šå·²å°å‡ºåˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” å…¨å”è©©æ•¸æ“šåˆ†æå·¥å…·")
    print("=" * 40)
    
    analyzer = QuantangshiAnalyzer()
    
    # è¼‰å…¥æ•¸æ“š
    analyzer.load_data()
    
    # åˆ†ææ•¸æ“š
    analyzer.analyze_authors()
    analyzer.analyze_volumes()
    
    # ç”Ÿæˆå ±å‘Š
    analyzer.generate_report()
    
    # å‰µå»ºå¯è¦–åŒ–
    try:
        analyzer.create_visualizations()
    except Exception as e:
        print(f"âš ï¸  å¯è¦–åŒ–å‰µå»ºå¤±æ•—: {e}")
        print("è«‹ç¢ºä¿å·²å®‰è£ matplotlib å’Œ seaborn")
    
    # å°å‡ºJSONæ•¸æ“š
    analyzer.export_json_data()
    
    print("\nğŸ‰ åˆ†æå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   - analysis_report.txt (åˆ†æå ±å‘Š)")
    print("   - quantangshi_analysis.png (å¯è¦–åŒ–åœ–è¡¨)")
    print("   - quantangshi_data.json (JSONæ•¸æ“š)")

if __name__ == "__main__":
    main() 